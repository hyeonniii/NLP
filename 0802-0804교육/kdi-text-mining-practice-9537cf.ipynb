{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 필요한 라이브러리 설치","metadata":{"id":"dPNmwDdoWUHl"}},{"cell_type":"code","source":"!pip install pdfminer.six","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:24:20.026049Z","iopub.execute_input":"2022-08-04T07:24:20.026666Z","iopub.status.idle":"2022-08-04T07:24:30.917307Z","shell.execute_reply.started":"2022-08-04T07:24:20.026544Z","shell.execute_reply":"2022-08-04T07:24:30.915662Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install konlpy\n!curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:24:30.920712Z","iopub.execute_input":"2022-08-04T07:24:30.921214Z","iopub.status.idle":"2022-08-04T07:24:43.104548Z","shell.execute_reply.started":"2022-08-04T07:24:30.921163Z","shell.execute_reply":"2022-08-04T07:24:43.103155Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. 데이터 불러오기","metadata":{"id":"5Plh17YhWUHo"}},{"cell_type":"code","source":"import pdfminer\nfrom pdfminer.high_level import extract_text\nfrom glob import glob\n\ndataname = input(\"저장할 파일들의 네이밍 규칙을 적어주세요 : \")\n# 17498_2 자리에 원하는 pdf 파일 이름을 넣어주세요!\npdfList = glob(\"../input/kdidata/*.pdf\")\ndocuments = []\nprint(pdfList)\n\nfor data_path in pdfList:\n    text = extract_text(data_path)\n\n    # 지저분한 텍스트 간단한 전처리\n    text = text.replace(\"\\x0c\", \"\")\n    text = text.replace('\\x00', \"\")\n    text = text.replace(\"\\n\", \"\")\n    text = text.replace('·', \"\")\n    documents.append(text)\n    \ndocuments","metadata":{"id":"2FJTvXqpWUHo","outputId":"23a2bda4-4b57-449e-a4a0-263521047c52","execution":{"iopub.status.busy":"2022-08-04T08:19:57.434724Z","iopub.execute_input":"2022-08-04T08:19:57.435694Z","iopub.status.idle":"2022-08-04T08:22:32.469412Z","shell.execute_reply.started":"2022-08-04T08:19:57.435650Z","shell.execute_reply":"2022-08-04T08:22:32.468220Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"len(documents)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:23:00.492279Z","iopub.execute_input":"2022-08-04T08:23:00.492713Z","iopub.status.idle":"2022-08-04T08:23:00.499928Z","shell.execute_reply.started":"2022-08-04T08:23:00.492679Z","shell.execute_reply":"2022-08-04T08:23:00.498901Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## 2. 전처리 함수 정의","metadata":{"id":"aQJN6mgwWUHr"}},{"cell_type":"code","source":"import re\nfrom konlpy.tag import Mecab; mecab = Mecab()\n\n# predefined의 경우 필요한 POS를 여기에 추가하시면 됩니다.\nKOR_POS = [\"NNP\"] # Korean\n#####\n\ndef text_cleaning(doc):\n    # 한국어를 제외한 글자를 제거하는 패턴.\n    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n    \n    # 특수문자를 제거하는 패턴.\n    #doc = re.sub(\"[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]\", \" \", doc)\n    \n    # 영문 빼고 모두 제거하는 패턴.\n    #doc = doc.replace(\"\\n\", \" \")\n    \n    return doc\n\ndef define_stopwords(path):\n    \n    SW = set()\n    # 불용어를 추가하는 방법 1.\n    # SW.add(\"있다\")\n    SW.add('NA')\n    SW.add(\"영어\")\n    SW.add(\"한국어\")\n    \n    # 불용어를 추가하는 방법 2.\n    # stopwords-ko.txt에 직접 추가\n    \n    with open(path, encoding=\"utf-8\") as f:\n        for word in f:\n            SW.add(word.strip())\n            \n    return SW\n\ndef text_tokenizing(doc, tokenizer):\n    \"\"\"\n    Input Parameter :\n    \n    doc - tokenizing 하는 실제 데이터.\n    tokenizer - token의 단위.\n    language - \"kor\"(한글) or \"eng\"(영어)\n    \n    \"\"\"\n    # 형태소 분석 결과를 return.\n    if tokenizer == \"noun\":\n        return [word for word in mecab.nouns(doc) if word not in SW and len(word) > 1]    \n\n    elif tokenizer == \"morph\":\n        return [word for word in mecab.morphs(doc) if word not in SW and len(word) > 1]\n\n    elif tokenizer == \"predefined\":\n        temp = mecab.pos(doc)\n        # predefined에 정의된 POS만 불러옵니다.\n        tokens = [token[0] for token in temp if token[1] in KOR_POS]\n        return [word for word in tokens if word not in SW and len(word) > 1]\n\n    elif tokenizer == \"word\":\n        return [word for word in doc.split() if word not in SW and len(word) > 1]","metadata":{"id":"KxY0gsUJWUHr","execution":{"iopub.status.busy":"2022-08-04T08:23:03.285660Z","iopub.execute_input":"2022-08-04T08:23:03.286094Z","iopub.status.idle":"2022-08-04T08:23:03.300828Z","shell.execute_reply.started":"2022-08-04T08:23:03.286056Z","shell.execute_reply":"2022-08-04T08:23:03.299869Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 불러온 데이터를 품사 태그를 붙여서 토크나이징합니다.","metadata":{"id":"gveifxIIWUHs"}},{"cell_type":"code","source":"import pickle\nimport os\nfrom pprint import pprint\nfrom tqdm.notebook import tqdm\n\n\nSW = define_stopwords(\"../input/korean-text-analysis/stopwords-ko.txt\")\n\n\nif os.path.exists(f'tokenized_set({dataname}).pk'):\n    with open(f'tokenized_set({dataname}).pk', \"rb\") as f:\n        tokenized_text = pickle.load(f)\n        \nelse:\n    cleaned_text = [text_cleaning(doc) for doc in documents]\n    # text_tokenizing 함수에는 (데이터, 품사, 언어) 정보가 들어가야 한다.\n    # 만약에 여러 개의 품사를 사용하려면, 품사에 \"predefined\" 를 넣으면 된다.\n    tokenized_text = [text_tokenizing(doc, \"noun\") for doc in cleaned_text]\n        \n    print(\"Cleaned Corpus : \", cleaned_text[0])\n    \n    with open(f'tokenized_set({dataname}).pk', \"wb\") as f:\n        pickle.dump(tokenized_text, f)\n\n    with open(f\"tokenized_text({dataname}).txt\", 'w') as f:\n        for doc in tokenized_text:\n            print(\" \".join(doc), file=f)\n        \nprint(\"\\n\\nTokenized Corpus : \", tokenized_text[0])","metadata":{"id":"PbK-1UrbWUHs","outputId":"a576d6f7-9630-4b3e-b298-7db89c3c0246","execution":{"iopub.status.busy":"2022-08-04T08:23:10.525248Z","iopub.execute_input":"2022-08-04T08:23:10.525690Z","iopub.status.idle":"2022-08-04T08:23:12.835223Z","shell.execute_reply.started":"2022-08-04T08:23:10.525644Z","shell.execute_reply":"2022-08-04T08:23:12.833992Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"len(tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:23:27.871641Z","iopub.execute_input":"2022-08-04T08:23:27.872172Z","iopub.status.idle":"2022-08-04T08:23:27.879315Z","shell.execute_reply.started":"2022-08-04T08:23:27.872122Z","shell.execute_reply":"2022-08-04T08:23:27.878435Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## 3. 빈도 분석하기.","metadata":{"id":"wdWnBmAKWUHt"}},{"cell_type":"code","source":"from collections import Counter\n\ntotal_tokens = [token for doc in tokenized_text for token in doc]\nprint(\"Number of Total tokens : \", len(total_tokens))\n\n# 각 token 별로 빈도를 계산해주는 Counter 객체.\ntoken_counter = Counter(total_tokens)\n\nwordInfo = dict()\nprint(\"\\n--Token : Freq--\")\nfor tags, counts in token_counter.most_common(50): # top 50개 출력.\n    wordInfo[tags] = counts\n    print (\"%6s : %d\" % (tags, counts))","metadata":{"id":"n-f1MI7zWUHt","outputId":"af6c4e3e-c9cf-40a5-e434-e4d02ac2a318","execution":{"iopub.status.busy":"2022-08-04T08:23:34.749245Z","iopub.execute_input":"2022-08-04T08:23:34.749643Z","iopub.status.idle":"2022-08-04T08:23:34.787217Z","shell.execute_reply.started":"2022-08-04T08:23:34.749612Z","shell.execute_reply":"2022-08-04T08:23:34.785915Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Histogram 그리기.","metadata":{"id":"ENsqSUoeWUHt"}},{"cell_type":"code","source":"# 그래프를 이쁘게 그리기 위한 코드입니다. 한글 글꼴을 추가합니다.\n\nimport matplotlib as mpl  # 기본 설정 만지는 용도\nimport matplotlib.pyplot as plt  # 그래프 그리는 용도\nimport matplotlib.font_manager as fm  # 폰트 관련 용도\nimport seaborn as sns\nmpl.rcParams['axes.unicode_minus'] = False\n\nsys_font=fm.findSystemFonts()\nprint(f\"sys_font number: {len(sys_font)}\")\nprint(sys_font)\n\nnanum_font = [f for f in sys_font if 'Nanum' in f]\nprint(f\"nanum_font number: {len(nanum_font)}\")\n\n!apt-get update -qq\n!apt-get install fonts-nanum* -qq\n\npath = '/usr/share/fonts/truetype/nanum/NanumBarunGothicBold.ttf'  # 설치된 나눔글꼴중 원하는 녀석의 전체 경로를 가져옵니다.\nfont_name = fm.FontProperties(fname=path, size=10).get_name()\nprint(font_name)\nplt.rc('font', family=font_name)\n\n# 현재 설정되어 있는 폰트 사이즈와 글꼴을 알아보자\n!python --version\ndef current_font():\n  print(f\"설정 폰트 글꼴: {plt.rcParams['font.family']}, 설정 폰트 사이즈: {plt.rcParams['font.size']}\")  # 파이썬 3.6 이상 사용가능하다\n        \nprint(current_font())\n\n# 여전히 글꼴이 보이지 않는 분들은, 런타임 -> \"다시 시작 및 모두 실행\" 을 눌러주세요!\n!rm -rf ~/.cache/matplotlib/*\n!fc-cache -fv","metadata":{"id":"Rn0OHnEAuvio","outputId":"5194eba4-559f-4406-a992-e0208a6ca00e","execution":{"iopub.status.busy":"2022-08-04T08:23:45.992567Z","iopub.execute_input":"2022-08-04T08:23:45.992977Z","iopub.status.idle":"2022-08-04T08:23:58.003815Z","shell.execute_reply.started":"2022-08-04T08:23:45.992947Z","shell.execute_reply":"2022-08-04T08:23:58.002222Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport platform\nfrom matplotlib import font_manager, rc\n\nplt.figure(figsize=(16, 12))\nplt.xlabel('주요 단어')\nplt.ylabel('빈도수')\nplt.grid(True)\n\nSorted_Dict_Values = sorted(wordInfo.values(), reverse=True)\nSorted_Dict_Keys = sorted(wordInfo, key=wordInfo.get, reverse=True)\n\nplt.bar(range(len(wordInfo)), Sorted_Dict_Values, align='center')\nplt.xticks(range(len(wordInfo)), list(Sorted_Dict_Keys), rotation='70')\nplt.savefig(\"freq_dist.png\")\nplt.show()","metadata":{"id":"aswpAIPoWUHu","outputId":"bdfc9393-3737-42ef-ec74-0bce17b80f4d","execution":{"iopub.status.busy":"2022-08-04T08:24:20.570728Z","iopub.execute_input":"2022-08-04T08:24:20.571239Z","iopub.status.idle":"2022-08-04T08:24:21.665115Z","shell.execute_reply.started":"2022-08-04T08:24:20.571197Z","shell.execute_reply":"2022-08-04T08:24:21.663977Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### WordCloud 그리기.","metadata":{"id":"pfUbkt40WUHu"}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nfrom PIL import Image\nimport numpy as np\n\nmask = np.array(Image.open(\"../input/korean-text-analysis/cloud.png\"))\n\nwordcloud = WordCloud(font_path=path,\n                      relative_scaling = 0.2,\n                      mask=mask,\n                      background_color='white',\n                      ).generate_from_frequencies(wordInfo)\nplt.figure(figsize=(16,16))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"wordcloud.png\")\nplt.show()","metadata":{"id":"9yIlJFSPWUHu","outputId":"996bad16-f9b1-4304-ae75-2f62c50c6e0b","execution":{"iopub.status.busy":"2022-08-04T08:24:28.205669Z","iopub.execute_input":"2022-08-04T08:24:28.206152Z","iopub.status.idle":"2022-08-04T08:24:29.195483Z","shell.execute_reply.started":"2022-08-04T08:24:28.206100Z","shell.execute_reply":"2022-08-04T08:24:29.193727Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## 4. TF-IDF를 통한 주요 단어 분석하기.","metadata":{"id":"17tlnRwGWUHv"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\n#TfidfVectorizer의 input으로 만들기 위한 전처리.\ntfidf_docs = [\" \".join(doc) for doc in tokenized_text]\n\ntfidf = TfidfVectorizer()\n\n# tfidf 형태로 변환.\nX_tfidf = tfidf.fit_transform(tfidf_docs)\n\nterms = tfidf.get_feature_names()\n\n# sum tfidf frequency of each term through documents\nsums = X_tfidf.sum(axis=0)\n\n# connecting term to its sums frequency\ndf = []\nfor col, term in enumerate(terms):\n    df.append( (term, sums[0,col] ))\n\nranking = pd.DataFrame(df, columns=['Term','TF-IDF'])\nrankInfo = ranking.sort_values('TF-IDF', ascending=False)[:50]\nprint(rankInfo)","metadata":{"id":"aBJIoSJEWUHv","outputId":"08451881-74d1-439b-b625-ab3f93b2c0cd","execution":{"iopub.status.busy":"2022-08-04T08:24:37.191218Z","iopub.execute_input":"2022-08-04T08:24:37.191669Z","iopub.status.idle":"2022-08-04T08:24:37.317715Z","shell.execute_reply.started":"2022-08-04T08:24:37.191631Z","shell.execute_reply":"2022-08-04T08:24:37.315700Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"len(tfidf_docs)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:24:45.488082Z","iopub.execute_input":"2022-08-04T08:24:45.489230Z","iopub.status.idle":"2022-08-04T08:24:45.495591Z","shell.execute_reply.started":"2022-08-04T08:24:45.489170Z","shell.execute_reply":"2022-08-04T08:24:45.494360Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF Histogram ","metadata":{"id":"I6MYh-NTWUHv"}},{"cell_type":"code","source":"tfidfInfo = dict()\n\nfor idx in range(len(rankInfo)):\n    term = rankInfo.iloc[idx][\"Term\"]\n    tfidf = rankInfo.iloc[idx][\"TF-IDF\"]\n    tfidfInfo[term] = tfidf\n\nplt.figure(figsize=(16, 12))\nplt.xlabel('주요 단어')\nplt.ylabel('TF-IDF')\nplt.grid(True)\n\nSorted_Dict_Values = sorted(tfidfInfo.values(), reverse=True)\nSorted_Dict_Keys = sorted(tfidfInfo, key=tfidfInfo.get, reverse=True)\n\nplt.bar(range(len(tfidfInfo)), Sorted_Dict_Values, align='center')\nplt.xticks(range(len(tfidfInfo)), list(Sorted_Dict_Keys), rotation='70')\nplt.savefig(\"tfidf_dist.png\")\nplt.show()","metadata":{"id":"wiC72id7WUHv","outputId":"8a190755-d4fa-4fa9-dee5-5d9f9bb20c09","execution":{"iopub.status.busy":"2022-08-04T08:25:12.519986Z","iopub.execute_input":"2022-08-04T08:25:12.521232Z","iopub.status.idle":"2022-08-04T08:25:13.641415Z","shell.execute_reply.started":"2022-08-04T08:25:12.521174Z","shell.execute_reply":"2022-08-04T08:25:13.640200Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### WordCloud ","metadata":{"id":"VE4bsKuTWUHw"}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nmask = np.array(Image.open(\"../input/korean-text-analysis/cloud.png\"))\nwordcloud = WordCloud(font_path=path,\n                      relative_scaling = 0.2,\n                      mask=mask,\n                      background_color='white',\n                      ).generate_from_frequencies(tfidfInfo)\nplt.figure(figsize=(16,16))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"tf-idf_wordcloud.png\")\nplt.show()","metadata":{"id":"xTfKNv2CWUHw","outputId":"ceccaf3e-af9b-4e35-d72d-ff5699764177","execution":{"iopub.status.busy":"2022-08-04T08:25:19.088846Z","iopub.execute_input":"2022-08-04T08:25:19.089592Z","iopub.status.idle":"2022-08-04T08:25:20.048206Z","shell.execute_reply.started":"2022-08-04T08:25:19.089549Z","shell.execute_reply":"2022-08-04T08:25:20.046833Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## 5. Topic Modeling을 위한 parameter setting 및 데이터 전처리.","metadata":{"id":"nfDkV0ulWUHw"}},{"cell_type":"code","source":"!pip install pyldavis==3.2.2","metadata":{"id":"ke7MCJokzvQn","outputId":"aa866536-3364-466d-a2b0-b81a1f2fa9ef","execution":{"iopub.status.busy":"2022-08-04T07:25:57.946581Z","iopub.execute_input":"2022-08-04T07:25:57.947459Z","iopub.status.idle":"2022-08-04T07:26:08.641552Z","shell.execute_reply.started":"2022-08-04T07:25:57.947401Z","shell.execute_reply":"2022-08-04T07:26:08.640297Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"start, end, step = [int(x) for x in input(\"토픽 개수를 입력하세요.(e.g. 2,10,1) : \").split(\",\")]","metadata":{"id":"1bp3vuQ2z1nl","outputId":"d518889f-7a02-4863-cf66-4fa34bb4377a","execution":{"iopub.status.busy":"2022-08-04T07:26:08.643231Z","iopub.execute_input":"2022-08-04T07:26:08.643570Z","iopub.status.idle":"2022-08-04T07:26:17.565258Z","shell.execute_reply.started":"2022-08-04T07:26:08.643539Z","shell.execute_reply":"2022-08-04T07:26:17.564119Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom operator import itemgetter\nfrom itertools import combinations\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.spatial.distance import pdist\nfrom scipy.spatial.distance import squareform\nfrom tqdm import tqdm_notebook\nfrom konlpy.tag import Mecab #Komoran #Mecab #Okt\nimport numpy as np\nimport string\nimport re\nimport warnings\nimport networkx as nx\nfrom gensim import corpora\nfrom gensim import models\nfrom gensim.models import TfidfModel\nimport pyLDAvis\nimport pyLDAvis.gensim\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\nwarnings.simplefilter(action='ignore')\n\n#tfidf로 토픽 모델링을 하는 경우에는 True.\ntfidf_mode = True\n# 각 토픽모델 결과마다 상위 몇개의 단어를 사용할지.\nNUM_TOPIC_WORDS = 30\n# semantic network할때, co-occurence count를 상위 몇개를 사용할지.\nNUM_WORD_COOCS = 100\n\nwrite_flag = True\n\n# 토픽 개수 지정\n# start 개수부터, end 개수까지 토픽 개수를 지정. 밑의 경우에는 K= 2, 3, 4, 5, 6, 7.\nx = range(start, end+1, step)\n\ndef build_doc_term_mat(documents):\n    \"\"\"주어진 문서 집합으로 문서-어휘 행렬을 만들어 돌려준다.\"\"\"\n    \n    print_log_msg(\"Building document-term matrix.\")\n    dictionary = corpora.Dictionary(documents)\n    temp_corpus = [dictionary.doc2bow(document) for document in documents]\n    \n    if tfidf_mode:\n        tfidf = TfidfModel(temp_corpus)\n        corpus = tfidf[temp_corpus]\n        \n    else:\n        corpus = temp_corpus\n\n    print_log_msg(\"Done.\")\n    \n    return corpus, dictionary\n    \n\n\ndef print_log_msg(msg):\n    \"\"\"로그 메시지를 출력한다.\"\"\"    \n    print(msg, flush=True)\n        \n        \ncorpus, dictionary = build_doc_term_mat(tokenized_text)","metadata":{"id":"sXVsoGxjWUHw","outputId":"df3f8d8c-2553-4682-c765-188c52640f97","execution":{"iopub.status.busy":"2022-08-04T07:26:17.566694Z","iopub.execute_input":"2022-08-04T07:26:17.567039Z","iopub.status.idle":"2022-08-04T07:26:18.060477Z","shell.execute_reply.started":"2022-08-04T07:26:17.567008Z","shell.execute_reply":"2022-08-04T07:26:18.059421Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## 6. Topic Modeling을 Coherence Score로 평가하기.","metadata":{"id":"hN5FFSVYWUHw"}},{"cell_type":"code","source":"from gensim.models import CoherenceModel # topic coherence score 계산하는 함수.\nfrom gensim.models.ldamodel import LdaModel # LDA\n\ndef compute_coherence(dictionary, corpus, texts, start=2, end=41, step=4):\n    \"\"\"\n    Input Parameter:\n    \n    dictionary - gensim dictionary\n    corpus - gensim corpus\n    texts - 토크나이징된 실제 문서\n    start, end, step - 실제 실험하는 토픽 개수. 아무것도 넣어주지 않으면 2부터 41까지 4씩 키워가며 자동으로 지정.\n    \"\"\"\n    \n    coherence_score_list = []\n    model_list = []\n    \n    # 모든 지정된 토픽 개수에 대해서 실행.\n    for num_topics in tqdm(range(start, end+1, step)):\n        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n                         iterations=300,\n                         random_state=42, alpha='auto') # passes를 높여주면 성능이 올라감. 보통 100~500 사이 지정.\n        \n        model_list.append(model)\n        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_score_list.append(coherence_model.get_coherence())\n        \n    return model_list, coherence_score_list\n\n\ndef print_topic_words(model):\n    \"\"\"토픽별 토픽 단어들을 화면에 인쇄한다.\"\"\"\n    \n    print_log_msg(\"Printing topic words.\")\n    \n    for topic_id in range(model.num_topics):\n        topic_word_probs = model.show_topic(topic_id, NUM_TOPIC_WORDS)\n        print(\"Topic ID: {}\".format(topic_id))\n\n        for topic_word, prob in topic_word_probs:\n            print(\"\\t{}\\t{}\".format(topic_word, prob))\n\n        print(\"\\n\")\n        \n\nmodel_path = \"LDA_model_\" + dataname + \"_(\" + str(min(x))+'-'+str(max(x)) +  \")topics.pk\"\ncsScore_path = \"coherence-Scores_\" + dataname + \"_(\" + str(min(x))+'-'+str(max(x)) +  \")topics.pk\"\n\n\nif write_flag:\n    model_list, coherence_scores = compute_coherence(dictionary=dictionary, corpus=corpus, \n                                                     texts=tokenized_text, start=start, end=end, step=step)\n\n    with open(model_path, 'wb') as f:\n        pickle.dump(model_list, f)\n        \n    with open(csScore_path, 'wb') as f:\n        pickle.dump(coherence_scores, f)\n    \n    with open(csScore_path[:-2]+\".txt\", 'w') as f:\n        for idx, cs in zip(range(start, end+1, step), coherence_scores):\n            print(\"# of Topics : \", idx, file=f)\n            print(\"Coherence Score with C_V : %.3f\" % cs, file=f)\n            \nelse:\n    if os.path.exists(model_path):\n        with open(model_path, 'rb') as f:\n            model_list = pickle.load(f)\n\n    if os.path.exists(csScore_path):\n        with open(csScore_path, 'rb') as f:\n            coherence_scores = pickle.load(f)","metadata":{"id":"Y_x4fXngWUHx","outputId":"dc2bd0e6-8c37-4a02-bfc6-e1dd40643a72","execution":{"iopub.status.busy":"2022-08-04T07:26:18.062415Z","iopub.execute_input":"2022-08-04T07:26:18.062976Z","iopub.status.idle":"2022-08-04T07:26:20.391633Z","shell.execute_reply.started":"2022-08-04T07:26:18.062942Z","shell.execute_reply":"2022-08-04T07:26:20.390266Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Coherence Score 계산 후 시각화. ","metadata":{"id":"MZUYIPj4WUHx"}},{"cell_type":"code","source":"plt.figure(figsize=(16, 16))\nlabel = \"Coherence Score(C_V)\"\nplt.plot(x, coherence_scores, label=label)\nplt.scatter(x, coherence_scores)\nplt.title(\"LDA_({}-{})topics\".format(str(min(x)), str(max(x))))\nplt.xticks(x)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence Score\")\nplt.legend(loc='best')\nplt.savefig(\"LDA_({}-{})topics.png\".format(str(min(x)), str(max(x))))\nplt.show()","metadata":{"id":"Y0TZ7f-xWUHy","outputId":"9e200f43-a88c-4ed4-dea0-d1be1e22233e","execution":{"iopub.status.busy":"2022-08-04T07:26:20.394330Z","iopub.execute_input":"2022-08-04T07:26:20.394827Z","iopub.status.idle":"2022-08-04T07:26:20.840540Z","shell.execute_reply.started":"2022-08-04T07:26:20.394777Z","shell.execute_reply":"2022-08-04T07:26:20.839622Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Coherence Score(CS)가 가장 높은 Topic 개수를 자동으로 찾아서, LDA를 진행함.\ncoherence_list = np.array(coherence_scores)\nmodel = model_list[np.argmax(coherence_list)]\nNUM_TOPICS = model.num_topics # CS가 가장 높은 토픽 개수.\nprint(\"Number of Topics : \", NUM_TOPICS)\nprint_topic_words(model)","metadata":{"id":"gQjGveCKWUHy","outputId":"3f8953dc-76e8-4d18-c869-d8a82f36eaa3","execution":{"iopub.status.busy":"2022-08-04T07:26:20.842370Z","iopub.execute_input":"2022-08-04T07:26:20.842806Z","iopub.status.idle":"2022-08-04T07:26:20.857165Z","shell.execute_reply.started":"2022-08-04T07:26:20.842763Z","shell.execute_reply":"2022-08-04T07:26:20.855842Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"pyLDAvis.enable_notebook()\n\nvis = pyLDAvis.gensim.prepare(model, corpus, dictionary)\nvis","metadata":{"id":"dtpezf7CWUHy","outputId":"da920fd5-6b3b-474f-c053-40192c0f6829","execution":{"iopub.status.busy":"2022-08-04T07:26:20.858676Z","iopub.execute_input":"2022-08-04T07:26:20.859438Z","iopub.status.idle":"2022-08-04T07:26:23.279577Z","shell.execute_reply.started":"2022-08-04T07:26:20.859392Z","shell.execute_reply":"2022-08-04T07:26:23.278228Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Semantic Network 그리기.","metadata":{"id":"etDvo-I-WUHz"}},{"cell_type":"markdown","source":"### 전체 단어에 대해서 토픽 모델링 상위 결과로 의미 연결망 만들기. ","metadata":{"id":"pkXjJDpcWUHz"}},{"cell_type":"code","source":"def get_topic_documents(model):\n    \"\"\"주어진 토픽 모델링 결과에서 토픽 문서를 생성하여 돌려준다.\"\"\"\n    \n    print_log_msg(\"Generating topic word documents.\")\n    topic_documents = []\n    \n    for topic_id in range(model.num_topics):\n        topic_document = []\n        topic_word_probs = model.show_topic(topic_id, NUM_TOPIC_WORDS)\n\n        for topic_word, prob in topic_word_probs:\n            topic_document.append(topic_word)\n            \n        topic_documents.append(topic_document)\n        \n    return topic_documents\n\ndef build_word_cooc_mat(model):\n    \"\"\"주어진 토픽 모델링 결과에서 어휘 공기 행렬을 생성하여 돌려준다.\"\"\"\n    \n    print_log_msg(\"Building topic word co-occurrence matrix.\")\n    word_cooc_mat = defaultdict(Counter)\n    topic_documents = get_topic_documents(model)\n    \n    for topic_document in topic_documents:\n        for word1, word2 in combinations(topic_document, 2):\n            word_cooc_mat[word1][word2] += 1\n            \n    return word_cooc_mat\n\ndef get_sorted_word_coocs(word_cooc_mat):\n    \"\"\"주어진 어휘 공기 행렬에서 공기 빈도로 역술 정렬된 행렬을 생성하려 돌려준다.\"\"\"\n    \n    print_log_msg(\"Sorting topic word occurrence.\")\n    word_coocs = []\n    \n    for word1, word2_counter in word_cooc_mat.items():\n        for word2, count in word2_counter.items():\n            word_coocs.append((word1, word2, count))\n            \n    sorted_word_coocs = sorted(word_coocs, key=itemgetter(2), reverse=True)\n    \n    \n    return sorted_word_coocs\n\n\ndef build_word_cooc_network(sorted_word_coocs):\n    \"\"\"토픽 단어 공기 네트워크를 생성하여 돌려준다.\"\"\"\n    \n    print_log_msg(\"Generating topic word co-occurrence network.\")\n    G = nx.Graph()\n\n    for word1, word2, count in sorted_word_coocs[:NUM_WORD_COOCS]:\n        G.add_edge(word1, word2, weight=count)\n        \n    return G\n\n\ndef return_log_scaled_lst(input_lst):\n    r_lst = map(np.log, input_lst)\n    try:\n        max_v = max(map(np.log, input_lst))\n        min_v = min(map(np.log, input_lst))\n        return map(lambda v: (v-min_v)/(max_v-min_v), r_lst) #min-max scaling in log-scale.\n    except: \n        print(input_lst)\n        \ndef print_log_msg(msg):\n    \"\"\"로그 메시지를 출력한다.\"\"\"\n    \n    print(msg, flush=True)\n\n\ndef plot_weighted_graph(sorted_word_coocs):\n    \"Plot a weighted graph\"\n    \n    print_log_msg(\"Generating topic word co-occurrence network.\")\n    \n    plt.figure(figsize=(16, 12))\n    plt.axis('off')\n    plt.title('Semantic Network from LDA')\n    plt.margins(x=0.05, y=0.05)\n    \n    G = nx.Graph()\n\n    for word1, word2, count in sorted_word_coocs[:NUM_WORD_COOCS]:\n        G.add_edge(word1, word2, weight=count)\n        \n    pos = nx.spring_layout(G, scale=3.)\n        \n    #measures = nx.in_degree_centrality(G)\n    #measures = nx.betweenness_centrality(G)\n    measures = nx.closeness_centrality(G)\n    node_weight_lst = return_log_scaled_lst([n_weight for n_weight in measures.values()])\n    edge_weight_lst = return_log_scaled_lst([e[2]['weight'] for e in G.edges(data=True)])\n    \n    \n    all_weights = []\n\n    for (node1,node2,data) in G.edges(data=True):\n        all_weights.append(data['weight']) # we'll use this when determining edge thickness\n \n    unique_weights = list(set(all_weights))\n    #print(sum(all_weights)) = 280\n    nodes = nx.draw_networkx_nodes(G, pos, node_size=list(map(lambda x: x*2000, node_weight_lst)),\n                                   node_color=\"green\",\n                                   alpha=0.65,\n                                   nodelist=list(measures.keys())\n                                  )\n    #nodes.set_norm(mcolors.SymLogNorm(linthresh=0.01, linscale=1))\n    \n    labels = {}\n    for node_name in measures.keys():\n        labels[str(node_name)] = str(node_name)\n    nx.draw_networkx_labels(G, pos, labels, font_size=16, font_family=font_name)\n\n\n    for weight in unique_weights:\n        weighted_edges = [(node1,node2) for (node1,node2,edge_attr) in G.edges(data=True) if edge_attr['weight']==weight]\n        width = weight #sum(all_weights) * 300.0\n        nx.draw_networkx_edges(G,pos,edgelist=weighted_edges,width=width, alpha=0.5)\n \n    print_log_msg(\"Drawing topic word network.\")\n    plt.savefig(\"network_plot.png\") \n    plt.show()\n    \n    return G\n\n\nword_cooc_mat = build_word_cooc_mat(model)\nsorted_word_coocs = get_sorted_word_coocs(word_cooc_mat)\nG = plot_weighted_graph(sorted_word_coocs)","metadata":{"id":"EeiFEG6fWUHz","outputId":"f9ef18f5-3b76-4a10-eda9-f9fbc494d41b","execution":{"iopub.status.busy":"2022-08-04T07:26:23.282168Z","iopub.execute_input":"2022-08-04T07:26:23.282818Z","iopub.status.idle":"2022-08-04T07:26:23.806292Z","shell.execute_reply.started":"2022-08-04T07:26:23.282766Z","shell.execute_reply":"2022-08-04T07:26:23.805453Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### 주어진 단어에 대해서 동시 등장(연어) 횟수가 높은 의미연결망 만들기. ","metadata":{"id":"IUuDlv2hWUHz"}},{"cell_type":"code","source":"from nltk import bigrams\n\n\"\"\"\n전체 corpus의 bigram을 구한 뒤, 분석 키워드를 중심으로 Semantic Network를 생성한다.\n\"\"\"\nfreq_analysis = False\n\ndef bigram_function(documents):\n    \n    bigram_corpus = []\n    \n    for doc in documents:\n        bigram_corpus += bigrams(doc)\n\n    return bigram_corpus\n\n\ndef frequency_checking(documents, N):\n\n    total_tokens = [token for doc in documents for token in doc]\n    print(\"Total tokens : \", len(total_tokens))\n    \n    text = nltk.Text(total_tokens, name='freqs')\n    print(\"Unique tokens : \", len(set(text.tokens)))\n    pprint(text.vocab().most_common(N))\n    \n\ndef bigram_frequency_checking(bigrams, N):\n\n    print(\"\\n\\nTotal bigrams : \", len(bigrams))\n\n    freq_dict = {}\n    \n    for bigram in bigrams:\n        freq_dict[bigram] = freq_dict.get(bigram, 0) + 1\n#         if bigram in freq_dict:\n#             freq_dict[bigram] += 1\n#         else:\n#             freq_dict[bigram] = 1\n        \n            \n    sorted_freq_dict = sorted(freq_dict.items(), key=itemgetter(1), reverse=True)\n\n    print(\"Unique bigrams : \", len(set(bigrams)))\n    \n    for k, v in sorted_freq_dict[:N]:\n        print(k, v)\n\n        \ndef build_sorted_keywords(bigrams, keyword, N):\n\n    freq_dict = {}\n    \n    for bigram in bigrams:\n        if keyword in bigram:\n            freq_dict[bigram] = freq_dict.get(bigram, 0) + 1\n            \n    sorted_freq_dict = sorted(freq_dict.items(), key=itemgetter(1), reverse=True)\n\n    if freq_analysis:\n        print(\"\\nNumber of bigrams : \", len(sorted_freq_dict))\n        \n        for k, v in sorted_freq_dict[:N]:\n            print(k, v)\n\n    return sorted_freq_dict[:N]   \n    \n    \ndef build_word_sim_network(sorted_word_sims, keyword):\n    \"\"\"어휘 유사도 네트워크를 생성하여 돌려준다.\"\"\"\n    \n    G = nx.Graph()\n    #max_count = 30\n\n    for bigram, count in sorted_word_sims:\n        word1, word2 = bigram[0], bigram[1]\n        \n        if word1 == keyword:\n#             if max_count == 0:\n#                 break\n#             max_count -= 1\n            #print(word1, word2, sim)\n            G.add_edge(word1, word2, weight=count)\n            \n        elif word2 == keyword:\n#             if max_count == 0:\n#                 break\n#             max_count -= 1\n            #print(word1, word2, sim)\n            G.add_edge(word2, word1, weight=count)\n            \n    T = nx.minimum_spanning_tree(G)\n\n    return T\n\n\ndef draw_network(G):\n    \"\"\"어휘 공기 네트워크를 화면에 표시한다.\"\"\"\n    \n    plt.figure(figsize=(16, 12))\n    nx.draw_networkx(G,\n                     pos=nx.spring_layout(G, k=0.8),\n                     node_size=1000,\n                     node_color=\"green\",\n                     alpha=0.65,\n                     font_family=font_name,\n                     with_labels=True,\n                     font_size=13)\n\n    plt.axis(\"off\")\n    plt.savefig(f\"semantic_network({dataname})_collocation.pdf\")\n    plt.show()\n\n\"\"\"\n어휘 유사도 행렬을 구성한 뒤 이를 네트워크로 시각화한다.\n\"\"\"\n\ndocuments = tokenized_text\nbigram_corpus = bigram_function(documents)\nN = int(input(\"\\nTop N? : \"))\nif freq_analysis:\n    frequency_checking(documents, N)\n    bigram_frequency_checking(bigram_corpus, N)\n\nkeyword = input(\"\\n키워드를 입력해주세요 : \")\nsorted_bigrams = build_sorted_keywords(bigram_corpus, keyword, N)\nG = build_word_sim_network(sorted_bigrams, keyword)\ndraw_network(G)","metadata":{"id":"uSIkLvXFWUHz","outputId":"2ca41b6b-4bd0-4029-92fd-c384d707fd64","execution":{"iopub.status.busy":"2022-08-04T07:26:23.807671Z","iopub.execute_input":"2022-08-04T07:26:23.808606Z","iopub.status.idle":"2022-08-04T07:26:31.749337Z","shell.execute_reply.started":"2022-08-04T07:26:23.808571Z","shell.execute_reply":"2022-08-04T07:26:31.748237Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### 주어진 단어에 대해서 동시 등장(co-occurence)가 높은 의미 연결망 만들기 ","metadata":{"id":"5swHmmmPWUH0"}},{"cell_type":"code","source":"\"\"\"\n전체 corpus의 co-occurence를 구한 뒤, 분석 키워드를 중심으로 Semantic Network를 생성한다.\n\"\"\"\n\ndef build_doc_term_mat(documents):\n    \"\"\"주어진 문서 집합으로부터 문서-어휘 행렬을 생성하여 돌려준다.\"\"\"\n    print(\"Building a Document-Term Matrix.\")\n    vectorizer = CountVectorizer(tokenizer=str.split, binary=True)\n    doc_term_mat = vectorizer.fit_transform(documents)   \n    words = vectorizer.get_feature_names()\n    \n    return doc_term_mat, words\n        \n    \ndef build_word_cooc_mat(doc_term_mat):\n    \"\"\"주어진 문서-어휘 행렬부터 어휘 공기 행렬을 생성하여 돌려준다.\"\"\"\n    \n    print(\"Building a co-occurrence matrix.\")\n    word_cooc_mat = doc_term_mat.T * doc_term_mat\n    word_cooc_mat.setdiag(0)\n    \n    return word_cooc_mat\n\n\ndef get_word_sim_mat(word_cooc_mat):\n    \"\"\"주어진 어휘 공기 행렬에 대하여 어휘 유사도 행렬을 구하여 돌려준다.\"\"\"\n    \n    print(\"Calculating Similarity matrix.\")\n    word_sim_mat = pdist(word_cooc_mat.toarray(), metric=\"correlation\")\n    word_sim_mat = squareform(word_sim_mat)\n    \n    return word_sim_mat\n\n\ndef get_sorted_word_sims(word_sim_mat, words):\n    \"\"\"주어진 어휘 유사도 행렬을 정렬하여 출력한다.\"\"\"\n    \n    print(\"Now Sorting..\")\n    word_sims = []\n    \n    for i, j in combinations(range(len(words)), 2):\n        sim = word_sim_mat[i, j]\n        \n        if sim == 0:\n            continue\n            \n        word_sims.append((words[i], words[j], sim))\n        \n    sorted_word_sims = sorted(word_sims, key=itemgetter(2), reverse=True)\n#     for k, v in sorted_word_sims[:N]:\n#         print(k, v)\n\n    return sorted_word_sims\n\ndef build_word_sim_network(sorted_word_sims, keyword, N):\n    \"\"\"어휘 유사도 네트워크를 생성하여 돌려준다.\"\"\"\n    \n    G = nx.Graph()\n    \n    max_count = N\n    for word1, word2, sim in sorted_word_sims:\n        \n        if word1 == keyword:\n            if max_count == 0:\n                break\n            max_count -= 1\n#            print(word1, word2, sim)\n            G.add_edge(word1, word2, weight=sim)\n            \n        elif word2 == keyword:\n            if max_count == 0:\n                break\n            max_count -= 1\n#            print(word1, word2, sim)\n            G.add_edge(word2, word1, weight=sim)\n        \n    T = nx.minimum_spanning_tree(G)\n\n    return T\n\n\ndef draw_network(G):\n    \"\"\"어휘 공기 네트워크를 화면에 표시한다.\"\"\"\n    \n    print(\"Done.\")\n    plt.figure(figsize=(16, 12))\n    nx.draw_networkx(G,\n                     pos=nx.spring_layout(G, k=0.8),\n                     node_size=1000,\n                     node_color=\"green\",\n                     font_family=font_name,\n                     with_labels=True,\n                     alpha=0.65,\n                     font_size=13)\n\n    plt.axis(\"off\")\n    plt.savefig(\"co-occurence_semantic_network.pdf\")\n    plt.show()\n    \n    \n\ndocuments = [\" \".join(tokens) for tokens in tokenized_text]\ndoc_term_mat, words  = build_doc_term_mat(documents)\nword_cooc_mat = build_word_cooc_mat(doc_term_mat)\nN = int(input(\"\\nTop N? : \"))\nkeyword = input(\"키워드를 입력해주세요 : \")\nword_sim_mat = get_word_sim_mat(word_cooc_mat)\nsorted_word_sims = get_sorted_word_sims(word_sim_mat, words)\nG = build_word_sim_network(sorted_word_sims, keyword, N)\ndraw_network(G)","metadata":{"id":"qymD2kTwWUH0","outputId":"aceeba47-1cf3-4239-8cd4-7ce3677c6e20","execution":{"iopub.status.busy":"2022-08-04T07:26:31.751013Z","iopub.execute_input":"2022-08-04T07:26:31.751490Z","iopub.status.idle":"2022-08-04T07:26:44.770096Z","shell.execute_reply.started":"2022-08-04T07:26:31.751456Z","shell.execute_reply":"2022-08-04T07:26:44.769052Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}